{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing the librtaries\nimport numpy as np\nimport nltk\nimport re\nimport gensim\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom gensim import corpora\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nimport heapq","metadata":{"id":"dPAV-0nFRQE7","execution":{"iopub.status.busy":"2023-06-30T07:54:32.427540Z","iopub.execute_input":"2023-06-30T07:54:32.428000Z","iopub.status.idle":"2023-06-30T07:54:34.123272Z","shell.execute_reply.started":"2023-06-30T07:54:32.427962Z","shell.execute_reply":"2023-06-30T07:54:34.122561Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install PyPDF2","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-06-30T07:54:34.124493Z","iopub.execute_input":"2023-06-30T07:54:34.124960Z","iopub.status.idle":"2023-06-30T07:54:51.524420Z","shell.execute_reply.started":"2023-06-30T07:54:34.124922Z","shell.execute_reply":"2023-06-30T07:54:51.523218Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[K     |████████████████████████████████| 232 kB 3.9 MB/s eta 0:00:01\n\u001b[?25hCollecting typing_extensions>=3.10.0.0\n  Downloading typing_extensions-4.7.0-py3-none-any.whl (33 kB)\nInstalling collected packages: typing-extensions, PyPDF2\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.7.4.3\n    Uninstalling typing-extensions-3.7.4.3:\n      Successfully uninstalled typing-extensions-3.7.4.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.4.1 requires typing-extensions~=3.7.4, but you have typing-extensions 4.7.0 which is incompatible.\narviz 0.11.2 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.7.0 which is incompatible.\naiobotocore 1.3.0 requires botocore<1.20.50,>=1.20.49, but you have botocore 1.20.53 which is incompatible.\u001b[0m\nSuccessfully installed PyPDF2-3.0.1 typing-extensions-4.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install docx2txt","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:54:51.526342Z","iopub.execute_input":"2023-06-30T07:54:51.526609Z","iopub.status.idle":"2023-06-30T07:54:58.367325Z","shell.execute_reply.started":"2023-06-30T07:54:51.526581Z","shell.execute_reply":"2023-06-30T07:54:58.366327Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting docx2txt\n  Downloading docx2txt-0.8.tar.gz (2.8 kB)\nBuilding wheels for collected packages: docx2txt\n  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3966 sha256=a89bb7d1e27ba39c811b3e9ab3ecae67a5946c14b4fe91ef0a75090a9f8f99b4\n  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\nSuccessfully built docx2txt\nInstalling collected packages: docx2txt\nSuccessfully installed docx2txt-0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport docx2txt\nfrom PyPDF2 import PdfReader\nfrom transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:54:58.368908Z","iopub.execute_input":"2023-06-30T07:54:58.369133Z","iopub.status.idle":"2023-06-30T07:55:04.425484Z","shell.execute_reply.started":"2023-06-30T07:54:58.369109Z","shell.execute_reply":"2023-06-30T07:55:04.424610Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to load and read a text document\ndef load_text_document(file_path):\n    with open(file_path, 'r') as file:\n        content = file.read()\n    return content\n\n# Function to load and read a Word document\ndef load_word_document(file_path):\n    content = docx2txt.process(file_path)\n    return content\n\n# Function to load and read a PDF document\ndef load_pdf_document(file_path):\n    content = \"\"\n    with open(file_path, 'rb') as file:\n        pdf = PdfReader(file)\n        num_pages = len(pdf.pages)\n        for page in range(num_pages):\n            content +=pdf.pages[page].extract_text()\n    return content\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:55:04.426586Z","iopub.execute_input":"2023-06-30T07:55:04.426987Z","iopub.status.idle":"2023-06-30T07:55:04.432809Z","shell.execute_reply.started":"2023-06-30T07:55:04.426948Z","shell.execute_reply":"2023-06-30T07:55:04.431678Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def process_document(file_path):\n    _, file_extension = os.path.splitext(file_path)\n    if file_extension == '.txt':\n        content = load_text_document(file_path)\n    elif file_extension == '.docx':\n        content = load_word_document(file_path)\n    elif file_extension == '.pdf':\n        content = load_pdf_document(file_path)\n    else:\n        print( \"Unsupported file format\")\n    return content  ","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:57:19.655388Z","iopub.execute_input":"2023-06-30T07:57:19.655962Z","iopub.status.idle":"2023-06-30T07:57:19.661504Z","shell.execute_reply.started":"2023-06-30T07:57:19.655924Z","shell.execute_reply":"2023-06-30T07:57:19.660459Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"file_path = \"../input/us-declaration-pdf-file/US_Declaration.pdf\"  # Replace with the actual path to your document\nprocess_document(file_path)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:57:27.341288Z","iopub.execute_input":"2023-06-30T07:57:27.341597Z","iopub.status.idle":"2023-06-30T07:57:27.390402Z","shell.execute_reply.started":"2023-06-30T07:57:27.341567Z","shell.execute_reply":"2023-06-30T07:57:27.389491Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\"Declaration of Independence\\nIN CONGRESS, July 4, 1776.  \\nThe unanimous Declaration of the thirteen united States of America,  \\nWhen in the Course of human events, it becomes necessary for one people to dissolve thepolitical bands which have connected them with another, and to assume among the powers of theearth, the separate and equal station to which the Laws of Nature and of Nature's God entitlethem, a decent respect to the opinions of mankind requires that they should declare the causeswhich impel them to the separation. We hold these truths to be self-evident, that all men are created equal, that they are endowed bytheir Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit\\nof Happiness.— \\x14That to secure these rights, Governments are instituted among Men, derivingtheir just powers from the consent of the governed,—  \\x14That whenever any Form of Government\\nbecomes destructive of these ends, it is the Right of the People to alter or to abolish it, and to\\ninstitute new Government, laying its foundation on such principles and organizing its powers in\\nsuch form, as to them shall seem most likely to effect their Safety and Happiness. Prudence,indeed, will dictate that Governments long established should not be changed for light andtransient causes; and accordingly all experience hath shewn, that mankind are more disposed to\\nsuffer, while evils are sufferable, than to right themselves by abolishing the forms to which theyare accustomed. But when a long train of abuses and usurpations, pursuing invariably the same\\nObject evinces a design to reduce them under absolute Despotism, it is their right, it is their duty,\\nto throw off such Government, and to provide new Guards for their future securit y.— \\x14Such has\\nbeen the patient sufferance of these Colonies; and such is now the necessity which constrainsthem to alter their former Systems of Government. The history of the present King of GreatBritain is a history of repeated injuries and usurpations, all having in direct object the\\nestablishment of an absolute Tyranny over these States. To prove this, let Facts be submitted to a\\ncandid world. \\nHe has refused his Assent to Laws, the most wholesome and necessary for the\\npublic good.He has forbidden his Governors to pass Laws of immediate and pressingimportance, unless suspended in their operation till his Assent should be obtained;and when so suspended, he has utterly neglected to attend to them.He has refused to pass other Laws for the accommodation of large districts of\\npeople, unless those people would relinquish the right of Representation in theLegislature, a right inestimable to them and formidable to tyrants only. He has called together legislative bodies at places unusual, uncomfortable, and distantfrom the depository of their public Records, for the sole purpose of fatiguing them into\\ncompliance with his measures.He has dissolved Re presentative Ho uses repeatedly , for opposing wit h manly\\nfirmness his invasions on the rights of the people.\\nHe has refused for a long time, after such dissolutions, to cause others to be\\nelected; whereby the Leg islative powers, incapable of Annihilation, have returned\\nto the People at lar ge for their exe rcise; the State r emaining in the me an time\\nexposed to all the dangers of invasion from without, and convulsions within.\\nHe has endeavou red to prevent the  population of these  States; for that pur pose\\nobstructing the L aws for Natural ization of Foreig ners; refusing  to pass others to\\nencourage their migrations hither, and raising the conditions of new\\nAppropriations of  Lands.\\nHe has obstructed the Administration of Justice, by refusing his Assent to Laws\\nfor establishing  Judiciary pow ers.\\nHe has made Judge s dependent on his Wil l alone, for the te nure of their off ices,\\nand the amount and  payment of t heir salaries.\\nHe has erected  a multitude of New  Offices, and se nt hither swarms of  Officers to\\nharrass our people, and eat out their substance.\\nHe has kept among us, in times of peace, Standing Armies without the Consent of\\nour legislature s.\\nHe has affected to render the Military independent of a nd superior to the Civil power.\\nHe has combined with others to subject us to a jurisdiction foreign to our\\nconstitution, and unacknowledged by our laws; giving his Assent to their Acts of\\npretended Legislation:\\nFor Quartering  large bodies of  armed troops amon g us:\\nFor protecting them, by a mock Trial, from punishment for any  Murders which\\nthey should c ommit on the Inha bitants of these Sta tes:\\nFor cutting off our Trade with all parts of the world:\\nFor imposing Taxe s on us without our Con sent: For deprivi ng us in many  cases,\\nof the benefits of T rial by J ury:\\nFor transporting us beyond Seas to be tried for pretended of fences\\nFor abolishing the free System of English L aws in a neighbouring Province,\\nestablishing therein an Arbitrary government, and e nlarging its Boundaries so asto render it at onc e an example and fi t instrument for intr oducing the same\\nabsolute rule into  these Colonies:\\nFor taking away our Charters, abolishing our most valuable L aws, and altering\\nfundamentally  the Forms of our G overnments:\\nFor suspending  our own Leg islatures, and de claring themse lves invested with\\npower to legislate for us in all cases whatsoever.\\nHe has abdicated Government here, by declar ing us out of his Protection and\\nwaging War ag ainst us.\\nHe has plundered our seas, ravaged our Coasts, burnt our towns, and destroy ed the\\nlives of our people.\\nHe is at this time transporting large Armies of foreign Mercenaries to compleat\\nthe works of death, desolation and tyranny , already begun with circumstances of\\nCruelty & pe rfidy scar cely para lleled in the most ba rbarous ages,  and totally\\nunworthy of the Head of a civilized nation.\\nHe has constrained our fellow Citizens taken Captive on the high Seas to bear\\nArms against their Country, to become the executioners of their friends and\\nBrethren, or to  fall themselves by  their Hands.\\nHe has excited domestic insurrections amongst us, and has endeavoured to bring\\non the inhabitants of our frontiers, the merciless Indian Savages, whose known\\nrule of warfare, is an undistinguished destruction of all ages, sexes and conditions. \\nIn every  stage of these O ppressions We have  Petitioned for Redr ess in the most humble  terms:\\nOur repeated Pe titions have been a nswered only  by repeate d injury. A Pr ince whose char acter is\\nthus marked by every ac t which may define a Tyra nt, is unfit to be the ruler of a free people. \\nNor have We been wanting in attentions to our Brittish brethren. We have warned them from\\ntime to time  of attemp ts by th eir legi slature t o extend an  unwarra ntable jur isdiction  over us. We\\nhave reminded them of the circumstances of our emigration and settlement here. We have\\nappealed to their native justice and magnanimity, and we have c onjured them by the ties of our\\ncommon kindred to disavow these usurpations, which, would inevitably interrupt our\\nconnections and correspondence. They too have bee n deaf to the voice of justice and of\\nconsanguinity. We must, therefore, acquiesce in the necessity , which denounces our Separation,\\nand hold them, as we hold the rest of mankind, Enemies in War, in Peace Friends. \\nWe, therefore, t he Representativ es of the united Sta tes of America, i n General Congr ess,\\nAssembled, appealing to the Supreme Judge of the world for the rectitude of our intentions, do,\\nin the Name, and by Authority of the g ood People of these Colonies, solemnly publish and\\ndeclare, That th ese United Colonie s are, and of Rig ht ought to be Fre e and Indepe ndent States;\\nthat they are Absolved from all Allegiance to the British Crown, and that a ll political connectionbetween them and the State of Great Britain, is and ought to be totally dissolved; and that as Freeand Independent States, they have full Power to levy War, conclude Peace, contract Alliances,\\nestablish Commerce, and to do all other Acts and Things which Independent States may of rightdo. And for the support of this Declaration, with a firm reliance on the protection of divineProvidence, we mutually pledge to each other our Lives, our Fortunes and our sacred Honor.[The 56 signatures on the Declaration were arranged in six columns: ] \\n[Column 1] Georgia:\\n   Button Gwinnett\\n   Lyman Hall\\n   George Walton \\n[Column 2] North Carolina:\\n   William Hooper\\n   Joseph Hewes\\n   John Penn\\n South Carolina:\\n   Edward Ru tledge\\n   Thomas Heyward, Jr.\\n  Thomas Lynch, Jr.\\n  Arthur Middleton \\n[Column 3] Massachusetts:\\n   John Hancock Maryland:\\n   Samuel Chase   William Paca   Thomas Stone   Charles Carroll of Carrollton Virginia:\\n   George Wythe   Richard Henry Lee   Thomas Jefferson   Benjamin Harrison   Thomas Nelson, Jr.   Francis Lightfoot Lee   Carter Braxton [Column 4] Pennsylvania:\\n  Robert Morris   Benjamin Rush\\n   Benjamin Fran klin\\n   John Morton   George Clymer\\n   James Smith\\n   George Taylor\\n   James Wilson\\n   George Ross\\n Delaware:\\n   Caesar Rodney\\n   George Read\\n   Thomas McKean \\n[Column 5] New York:\\n   Wi lliam Floyd\\n   Philip Livingston\\n   Francis L ewis\\n   Lewis Morris\\n New Jersey:\\n   Richard Stockton\\n   John Witherspoon\\n   Francis Hopkinson\\n   John Hart\\n   Abraham Clark \\n[Column 6] New Hampshire:\\n   Josiah Bartlett\\n   William Whipple\\n Massachusetts:\\n   Samuel Adams\\n   John Adams\\n   Robert Treat Paine\\n   Elbridge Gerry\\n Rhode Island:\\n   Stephen Hopkins\\n   William Ellery\\n Connecticut:\\n   Roger Sherman\\n   Samuel Huntington\\n   William Williams\\n   Oliver Wolcott\\n New Hampshire:\\n Matthew Thornton\\n \""},"metadata":{}}]},{"cell_type":"code","source":"file_path = \"../input/elonmusk/Elon Musk.txt\"  # Replace with the actual path to your document\ncontent = process_document(file_path)\ncontent","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:58:04.068516Z","iopub.execute_input":"2023-06-30T07:58:04.068835Z","iopub.status.idle":"2023-06-30T07:58:04.076880Z","shell.execute_reply.started":"2023-06-30T07:58:04.068806Z","shell.execute_reply":"2023-06-30T07:58:04.075834Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'Elon Musk is a 47-year-old South African entrepreneur, investor, and inventor who has been fascinated with science and technology his entire life. His accolades include being the founder of SpaceX and co-founder of businesses such as Tesla, Paypal, and the Boring Company. Through his life as a man at the forefront of technological advancements, he has had many failings and critics along the way, particularly in the initial stages of his SpaceX project, from huge organizations such as NASA. Despite all this, his company managed to land a rocket booster vertically, for the first time, allowing boosters to be reused and revolutionizing the industry.\\n\\nOne leadership trait which is apparent is his vision, dedication, and perseverance. This means that, as a leader, he is laser-focused on the task at hand and is not phased by the possibility of failure due to having the desire and drive to do what he needs to do. An example of his drive and perseverance would be the fact that he was continuously investing and running Tesla through a long period of making losses. The reason for this is focused on the current objective and understands that it may not go smoothly at the initial stages but there are times when it is necessary to keep yourself and your people in the fight to reap the benefits later.\\nFollowing on, Musk has the ability to pass his vision on to those who work for him and then persuade them to become fully invested in his vision. This is a crucial trait of a successful leader, particularly in an innovative field, due to the fact that it means he is able to get his workforce fully behind any project regardless of apparent absurdity and encourage his team to work towards a common goal and invest themselves fully in the aim of the company. Without this, it is unlikely he would have been so successful in his ventures and tasks would not have been carried out so thoroughly.\\n\\nOne of Musk’s most important assets is that he is a social architect. By inspiring his employees on each of his ventures, he has been able to mobilize his workforce and encourage a new group identity. It is thought that because of this special philosophy, he has created a unique standard of excellence amongst his workforce. In addition to this, he is committed to leading this highly motivated workforce from the front which is clear in a statement made by him saying “For my part, I will never give up and I mean never”. He truly believes in his futuristic ideas and therefore is able to translate that belief to the rest of his company and instill a forward-thinking and creative culture.'"},"metadata":{}}]},{"cell_type":"code","source":"file_path = \"../input/thesis/CHAPTER 1.docx\"  # Replace with the actual path to your document\ncontent= process_document(file_path)\n\nprint(content)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:58:20.165633Z","iopub.execute_input":"2023-06-30T07:58:20.165966Z","iopub.status.idle":"2023-06-30T07:58:20.310235Z","shell.execute_reply.started":"2023-06-30T07:58:20.165936Z","shell.execute_reply":"2023-06-30T07:58:20.309502Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"LIVERPOOL JOHN MOORE’S UNIVERSITY\n\nFaculty of Engineering and Technology\n\n\n\n\n\nFake image forgery detection using deep learning\n\n\n\nBy\n\n\n\nShubhangi Trivedi\n\n\n\nSupervisor: Mr Bikash Santra\n\n\n\n\n\n\n\nFinal Thesis Report\n\n\n\n\n\n\n\nSubmitted on\n\n\n\nDecember 2021\n\n\n\n\n\nA thesis submitted in accordance with the statutes and regulations of the University in part fulfilment of the requirements for the Degree of Master of Science\n\nDEDICATION\n\n\n\nThis work is dedicated to my son, who has made me feel better and more fulfilled. I am now stronger and more determined than I could ever imagine.  I saw you grow as this project progressed, and that satisfaction is indescribable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nACKNOWLEDGEMENT\n\n\n\nIt is my great joy to express my gratitude to the Almighty God for providing me with a sound mind in which to complete my MS Research Report. \n\nMr. Bikash Santra, my thesis supervisor, deserves special recognition for his unwavering support and patience during this research. He was always accessible to answer any questions I had, and having his help ensured that my research was completed on time.\n\nFinally, I'd want to thank my wonderful and supporting husband, Ankit. Your words of support during difficult times were much appreciated and recorded. Thank you from the bottom of my heart.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABSTRACT\n\n\n\nNow we depend a lot on multimedia content and with the availability of increased need of content, there is a lot of intentional manipulation is happening. With advancement, we have multiple options available in the market for image editing. Image tempering has become very easy and detecting these forgeries through naked eyes has become a difficult task. Thus, with such advancements in technology, it gets very difficult to judge the credibility of any image available, and such images are accepted by the public without questioning their integrity. So, in this dissertation, we will try to detect real and forged images using deep learning models. For training purposes, we have used the publicly available dataset CASIA V2.0.  We used the passive image forgery detectors that would use ELA information of the image to classify an image into fake or real. When an image is forged it is generally the metadata information of the image that gets manipulated and on a portion of the image, an error is introduced due to recompression that happens on each image save.\n\nBy using error level analysis, we will calculate the difference between compression levels between different images. We will be using different state of Art models for feature extraction we used ELA technique. We have also used various pre-processing techniques like denoising, image augmentation that helped in achieving maximum accuracy.\n\nWe have used custom CNN model also along with state of art models like VGG16and RESNET50, but we achieved the maximum validation accuracy of 94%   with custom CNN model and that mode we have considered the final model for image forgery detection.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\tTABLE OF CONTENT\t\n\n\n\n\tDEDICATION\tii\n\n\tACKNOWLEDGEMENT\tiii\n\n\tABSTRACT\tiv\n\n\tTABLE OF CONTENT\tv\n\n\tLIST OF FIGURES\tviii\n\n\tLIST OF TABLES\tx\n\n\tLIST OF ABBREVIATIONS\txi\n\n\t\tCHAPTER 1: INTRODUCTION\t1\n\n\t1.1 Background\t1\n\n\t1.2 Problem statement\t4\n\n\t1.3 Aims and Objectives\t5\n\n\t1.4 Significance of study\t5\n\n\t1.5 Scope of Study\t6\n\n\t1.6 Structure of Study\t6\n\n\t\tCHAPTER 2: LITERATURE REVIEW\t8\n\n\t2.1 Introduction\t8\n\n\t2.2 Image Compression\t9\n\n2.2.1 Lossless\t10\n\n2.2.2 Lossy\t11\n\n\t2.3 Image Formats\t11\n\n2.3.1 JPEG\t12\n\n2.3.2 TIFF\t12\n\n2.3.3 BMP\t12\n\n\t2.4 Methods of detecting image tampering:\t13\n\n2.4.1 Physics-based techniques\t13\n\n2.4.2 Format Based\t15\n\n2.4.3 Camera-based techniques:\t17\n\n2.4.4 Geometric based techniques:\t19\n\n\t2.5 Advance Image tampering detection\t19\n\n2.5.1 ELA (Error Level Analysis):\t19\n\n2.5.2 CNN\t21\n\n\t2.6 Comparative study of existing forgery detections methods\t23\n\n\t2.7 Summary\t27\n\n\t\tCHAPTER 3: RESEARCH METHODOLOGY\t28\n\n\t3.1 Introduction\t28\n\n\t3.2 Methodology\t28\n\n3.2.1 Data Set Description\t28\n\n3.2.2 Pre-Processing\t30\n\n3.2.3 Proposed Method\t30\n\n3.2.4 Classification\t36\n\n3.2.5 Evaluation Metrics\t36\n\n\t3.3 Summary\t37\n\n\t\tCHAPTER 4: ANALYSIS AND IMPLEMENTATION\t38\n\n\t4.1 Introduction\t38\n\n\t4.2 Data set description\t39\n\n\t4.3 EDA\t40\n\n4.3.1 Raw Image comparison\t40\n\n4.3.2 Average Image and Contrast:\t40\n\n4.3.3 Eigenimages:\t42\n\n\t4.3 Pre-Processing\t44\n\n4.3.1 Image Augmentation\t45\n\n4.3.2 Denoising\t47\n\n4.3.3 K Fold Cross Validation\t52\n\n4.3.4 ELA\t53\n\n4.3.5 Class Imbalance\t55\n\n\t4.4 Model Building\t56\n\n4.4.1 VGG16\t56\n\n4.4.2 RESNET50\t57\n\n4.4.3 Custom CNN Model\t59\n\n\t4.5 Summary\t61\n\n\t\tCHAPTER 5: RESULTS AND DISCUSSION\t62\n\n\t5.1 Introduction\t62\n\n\t5.2 Image Pre-processing Results\t62\n\n5.2.1 Wavelet Denoising Results:\t62\n\n\t5.3 Model Building\t64\n\n5.3.1 Model Building using transfer learning via VGG16\t64\n\n5.3.2 Model Building using transfer learning via RESNET50\t66\n\n5.3.3 Model Building using transfer learning using custom model\t68\n\n\t5.4 Summary\t70\n\n\t\tCHAPTER 6: CONCLUSION AND FUTURE RECOMMENDATION\t71\n\n\t6.1 Introduction\t71\n\n\t6.2 Conclusion\t71\n\n6.2.1 Conclusion\t71\n\n6.2.2 Limitations of model\t72\n\n6.2.3 Learnings\t72\n\n\t6.3 Future Recommendation\t73\n\n\t6.4 Summary\t74\n\n\tREFERENCE\t75\n\n\tAPPENDIX A – RESEARCH PROPOSAL\t79\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLIST OF FIGURES\n\n\n\nFigure 1.1 Image forgery techniques\t2\n\nFigure 1.2 Forged Images Examples, the first example shows a change of person (image splicing), the second image shows the addition of fountain (Copy-move) and the third image shows the removal of a person(removal) (Diallo et al., 2020)\t3\n\nFigure 1.3 Splicing\t4\n\nFigure 1.4 Example of Image retouching (Armas Vega et al., 2020)\t4\n\nFigure 1.5 Flow Chart for proposed approach\t6\n\nFigure 2.1 Digital Photography\t8\n\nFigure 2.2 Line profile A–extended A's stream of zeros (black) will easily give significant compression savings, however, line profile B–seemingly B's random profile will see a modest benefit. Data patterns and repetitions are exploited in lossless compression. Tan e\t10\n\nFigure 2.3 Lossy compression artifacts (a, b, and c): the example image is saved at successively greater levels of compression, initially at 1:1, then second image with 1:10, and final image with 1:30 of compression level. The image is quite legible even at 1:30, though there are apparent visual flaws; (d) the image with 1:30 compression level when zoomed 5 times. image is showing visible smudging, which is a really important feature of the lossy compression of the JPEG. (Tan, 2006)\t11\n\nFigure 2.4 A recognised counterfeit of Jane Fonda & John Kerry sharing the stage at an anti-war rally is shown above. Kerry's estimated light direction is 123, while Fonda's light direction is 86 estimated. An actual photograph of Richard Nixon and Elvis Presley (Johnson, 2005)\t14\n\nFigure 2.5 Block diagram of the ELA method used by (Afsal Villan et al., 2017)\t16\n\nFigure 2.6  a. Shows unchanged region of a tampered image as blank region and the changed or forged region as shaded region.\t17\n\n2.7 Block diagram of DCT method (Alahmadi et al., 2017)\t17\n\nFigure 2.8 Original Image\t18\n\nFigure 2.9 Tampered region detected using CFA pattern estimation (Regina et al., 2010)\t18\n\nFigure 2.10 a) the actual image, b) Forged image with tampering done around eyes and lips and a flower added on the hat, and c) ELA transformed image for the forged image. (Jeronymo et al., 2017)\t20\n\nFigure 2.11 a) shows the original image, b) shows the forgery, with a zeppelin in the background and c) shows ELA for the tampered image. (Jeronymo et al., 2017)\t20\n\nFigure 3.1 Proposed Approach\t28\n\nFigure 3.2 Example of images captured from CASIA v 2.0 dataset\t29\n\nFigure 3.3 Architecture of Convolutional Neural Network\t31\n\nFigure 3.4 Element-wise multiplication and summation in CNN\t31\n\nFigure 3.5 Pooling Types\t32\n\nFigure 3.6 Fully Connected Layer (Tammina, 2019)\t33\n\nFigure 3.7 VGG Architecture (Tammina, 2019)\t34\n\n3.8 Comparative diagram of Conventional Machine Learning and Transfer Learning\t35\n\nFigure 4.1 Flow chart for final proposed approach of training the model\t38\n\nFigure 4.2 Samples of Authentic and Tampered images\t40\n\nFigure 4.3 Average image of fake and real images\t41\n\nFigure 4.4 Contrast between real and fake images average values\t42\n\nFigure 4.5 Eigen Images for Real Images\t43\n\nFigure 4.6 Eigen Images for Fake Images\t44\n\nFigure 4.7 Code snippet of Data generator used\t46\n\nFigure 4.8 Result of Data Augmentation performed on one sample image\t47\n\nFigure 4.9 Probability Density Function for Salt and Pepper noise. (Ahmad Jawad Khan Muhammad Salah Ud Din Iqbal, 2019)\t48\n\nFigure 4.10 Traditional Filters for Denoising techniques (Nidhi Mantri, 2021)\t49\n\nFigure 4.11 Block diagram for denoising approach (Bnou et al., 2020)\t50\n\nFigure 4.12 Code Snippet of denoising implementation\t51\n\nFigure 4.13 Code Snippet of denoising implementation with reduced threshold\t51\n\nFigure 4.14 Code snippet of K fold implementation in keras\t52\n\n4.15 Code snippet of method used to apply ELA on all images\t53\n\nFigure 4.16 Code snippet of ELA conversion on all the Au folder images\t54\n\nFigure 4.17 ELA output on Real and Fake image\t55\n\nFigure 4.18 Code snippet of model used for transfer learning using VGG16\t57\n\nFigure 4.19 Architecture of CNN with transfer learning via VGG16 model used\t57\n\nFigure 4.20 Code snippet of model used for transfer learning using RESNET50\t58\n\nFigure 4.21 Architecture of CNN with transfer learning via RESNET50 model used\t59\n\nFigure 4.22 Code snippet of model used for transfer learning using RESET50\t60\n\nFigure 4.23 Architecture of custom CNN model\t60\n\nFigure 5.1 Denoising impact on real image\t63\n\nFigure 5.2 Denoising impact on Fake Image\t64\n\nFigure 5.3 Training loss Vs Validation loss and Training accuracy Vs Validation Accuracy\t66\n\nFigure 5.4 Confusion Matrix for VGG 16\t66\n\nFigure 5.5 Training loss Vs Validation loss and Training accuracy Vs Validation Accuracy\t67\n\nFigure 5.6 Confusion Matrix for RESNET50\t68\n\nFigure 5.7 Training loss Vs Validation loss and Training accuracy Vs Validation Accuracy\t69\n\nFigure 5.8 Confusion Matrix for custom CNN model\t70\n\n\n\n\n\n\n\n\n\nLIST OF TABLES\n\n\n\nTable 2.1 Comparative study of image forgeries techniques\t23\n\nTable 3.1 Statistical information about the Tampered image (Jing Dong, 2013)\t29\n\nTable 5.1 PSNR comparison of wavelet denoising\t62\n\nTable 5.2 Training and validation accuracy for 20 epochs\t65\n\nTable 5.3 Training and validation accuracy for 20 epochs\t66\n\nTable 5.4 Training and validation accuracy for 30 epochs\t68\n\nTable 6.1 Comparison of performance of various model implemented\t72\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLIST OF ABBREVIATIONS\n\n\n\nAMFE - APPROXIMATED MACHADO FRACTIONAL ENTROPY\n\nCASIA - CHINESE ACADEMY OF SCIENCES' INSTITUTE OF AUTOMATION.\n\nCCD - CHARGE-COUPLED DEVISE\n\nCFA – COLOR FILTER ARRAY\n\nCFM – COLOUR FILTER MOSAIC\n\nCMFD - COPY MOVE FORGERY DETECTION\n\nCMOS - COMPLEMENTARY METAL-OXIDE SEMICONDUCTOR\n\nCNN- CONVOLUTIONAL NEURAL NETWORK\n\nDCT - DIGITAL COSINE TRANSFORM\n\nDNN - DEEP NEURAL NETWORKS \n\nDL -DEEP LEARNING\n\nELA – ERROR LEVEL ANALYSIS\n\nEDA- EXPLORATORY DATA ANALYSIS\n\nFPR - FALSE POSITIVE RATE\n\nFCN – FULLY CONVOLUTIONAL NETWORK\n\nGAN - GENERATIVE ADVERSARIAL NETWORK\n\nMFCN - MULTI-TASK FULLY CONVOLUTIONAL NETWORK\n\nPCA - PRINCIPAL COMPONENT ANALYSIS\n\nPCT - PRINCIPAL COMPONENT TRANSFORMATION\n\nPDF – PROBABILITY DENSITY FUNCTION\n\nRNN- RECURRENT NEURAL NETWORKS \n\nSIFT - SCALE-INVARIANT FEATURE TRANSFORM\n\nSVD: SINGULAR VALUE DECOMPOSITION\n\nSVM - SUPPORT VECTOR MACHINE\n\nSURF- SPEEDED UP ROBUST FEATURES\n\nTIFF - TAGGED IMAGE FILE FORMAT\n\nVGG - VISUAL GEOMETRY GROUP FROM OXFORD\n\n\n\n\nCHAPTER 1: INTRODUCTION\n\n\n\n1.1 Background\n\n\n\nThe use of digital media has become a part of our daily life with the advent of technology- handheld devices and fast flow of information. Internet and connectivity have exposed us to Exabyte of data especially graphical data such as images, videos, etc. Mobile sharing of images has become so common throughout the world that people have by default exposed themselves to cyber predators who keep a close watch on a typical user's activity. A typical mobile/computer user is not aware of what these predators can do with the images they are sharing. These predators use different types of techniques of changing the data within the media and use the exploited media for malicious purposes.\n\nThis situation portrays a definitive need for a comprehensive solution to protect the end user from such digital predators. For providing a flawless solution to the problem, developers/security experts need to have some sort of basis to find out whether the given sample media (photo or a video) is a tampered one or the original one.\n\nThe developer may simply ask two questions to determine the authenticity of a given sample image. Firstly, a developer may ask whether the image still depicts the captured original scene. Secondly, he/she may ask whether the image was captured from the device from which it is claimed to be generated. The second question is of prime importance as it allows determining definitive information (user/device which generated the media) about the source of the image. Since it is never easy to determine the actual source of the image, it becomes a great challenge to know anything about the original image.\n\nThe developer, therefore, needs to blindly follow what is available in his/her hands i.e., Tampered/original Image. Security experts have classified two methods to overcome challenges related to source determination, first is active in which information acquired from the source (camera, computer, etc.) is exploited to determine the authenticity of the available information. The second is passive in which no information of source is available for exploitation only the media available at disposal has to be checked directly. Types of image forgery detection techniques are shown in Figure 1.1.\n\nAn active approach is dependent upon a trustworthy camera i.e. A device that somehow grants a digital signature/watermark to the image automatically. If any cyber predator tries to tamper with the image, the act can easily be traced as the watermark/signature gets tampered with. But the active method bears a major drawback as this method requires a trustworthy camera with a standard protocol to provide a watermark to every image which is captured by it; however, such a requirement is next to impossible as providing a fixed watermark as a standard practice by every manufacturer is not feasible. Hence, this method is limited to a very scarce number of scenarios.\n\n\n\nFigure 1.1 Image forgery techniques\n\nSuch problems can easily be overcome by the use of techniques that do not require any prior information about the image and are hence called passive techniques. These techniques work on the principle of determining traces of data left whenever an image is acquired, compressed, stored, or modified. When such features of data are analyzed, it becomes easy to check for any kind of incongruence in the available data.\n\nA typical image can be edited in a variety of ways and therefore, poses a challenge for developers/security experts. An image can be edited for various reasons such as for the improvement of image quality etc. such editing is known as innocent editing, in which the available information becomes more streamlined. On the contrary, when the information is changed by hiding or adding something then such editing is known as malicious editing which is of prime interest for security experts.\n\n\n\n\n\nImage editing is broadly classified into three major categories, these categories comprise both operators i.e., innocent and malicious. The categories are Enhancement, Geometric and Content Modification. Here enhancement operator is primarily used for innocent editing, while other 02 categories (Geometric and Content modification) are intended for malicious attacks.\n\nEnhancement techniques involve basic features such as color modification, contrast adjustments, etc. Geometric modifications involve zoom, rotation, cropping, etc. finally content modification involves copy-move, cut and paste, etc. From the above explanation, it can easily be sought those the latter two techniques will one way or the other involve malicious editing.\n\n\n\nFigure 1.2 Forged Images Examples, the first example shows a change of person (image splicing), the second image shows the addition of fountain (Copy-move) and the third image shows the removal of a person(removal) (Diallo et al., 2020)\n\n\n\nMost common malicious attacks involve cropping, cut-paste, etc. as these techniques allow the forger to remove the content of an image and paste it somewhere else, be it any media. Figure 1.2 shows an example of the most common type of forgery. The most common categories of image forgeries are:\n\nCopy-Move Forgery: This kind of forgery involves transforming an original image by tampering with its contents by the use of editing features such as moving and copying. This kind of forgery becomes handy for forgers who want to make use of images in carrying out their political agendas, spreading misinformation, or creating illusions among the audience. This kind of forgery involves no change in the color and background of the original image. Many tools have been developed to detect such a forgery.\n\n\n\nImage Splicing: This kind of forgery mainly involves merging the contents of the same image or different images to compose a forged image. This technique enables the forger to change the backgrounds of the image or use a background of the different images in the subject image. This technique also allows enhancing the field of vision of any particular image. This technique provides greater flexibility to the forger and allows the forger to make multiple changes in a single image. The detection of this technique is a cumbersome task but can be identified if the forger has left any shadows, reflections in a given image. Figure 1.3 shows an example of image splicing.\n\n\n\n\n\nFigure 1.3 Splicing\n\n\n\nImage Retouching: This method allows altering common features of any image such as changing the color of the image, changing the scale of a given image, stretching, or squeezing a particular part of an image. Detection of this technique is very difficult as it involves multiple changes in multiple areas of a given image. Image retouching can be detected by detecting different lighting conditions in a spliced image. Figure 1.4 shows an example of image retouching.\n\n\n\nFigure 1.4 Example of Image retouching (Armas Vega et al., 2020)\n\n1.2 Problem statement\n\n\n\nWhen talked about image forgery using copy-move techniques, then splicing is considered difficult to be detected. Spicing is an operation in which a portion of one image is copied by an attacker and pasted on another image thus altering the image. What further makes its recognition difficult is that this copy-move is also followed by some level of compression, with some blur effect and then smoothening of boundaries. Thus, detecting forgery in the case of splicing is comparatively tricky than forgery techniques.\n\nELA is the best technique that uses the difference between compression levels among different regions of the image. Similarly, deep learning models are considered best for image recognition. Hence in this study we will try to use CNN for the identification of forgery.\n\n\n\n1.3 Aims and Objectives\n\n\n\nThe main aim of the research is to identify tampered images correctly using the CASIA V 2.0 dataset which involves the use of a Convolutional Neural Network for training purposes.\n\nThe objectives of the study based on the aims are following:\n\nTo identify the best preprocessing techniques that help in identifying image forgeries better and can generalize image effectively.\n\nTo identify if ELA can help in effective feature extraction\n\nTo identify the best state of art model that can help in detecting image forgeries with better accuracy.\n\nTo give overview different type of techniques that are currently used for image forgery.\n\n\n\n1.4 Significance of study \n\n\n\nThe result of this study will help in identifying the image forgery or tampering with high accuracy and precision. With the increased usage of social media, incidences of image manipulation are increasing and with advancement, in editing technology, these manipulations have become very hard to detect via the traditional approach and have started to erode the trust in digital content. This phenomenon demands the need for a way to help us verify the truthfulness and credibility of the image. Though color modification, contrast adjustment, and filtering are part of innocent image editing, geometric modification and content modification lead to malicious image editing.\n\nIf a forged image is spliced or copy-move, then interpolation is a necessary step. When a forged image contains areas from different sources, or another part of the same image, rescaling and /or rotation are often involved. In general, these changes are mostly done on a part of the image, not on the entire image. Thus, dividing the image into multiple blocks, and then detecting the changes compared to neighboring blocks will help us to detect the manipulated area. Apart from this whenever an image has been tampered the tampering process introduces a mild level of compression and hence compression can also help in identifying tampering. There is a significant level of change that may occur to image metadata also where the source of images is different. Hence by using this understanding of image forgery, we can develop a model that can help us identify image forgery. \n\nIn many cases, images are proof of medical reports and crime scene evidence and such forged image can result in loss of life or escape of convict. Thus, by this paper, we will try to create a model that could detect image forgery effectively and efficiently and thus we can easily verify the authenticity of an image.\n\n\n\n1.5 Scope of Study\n\n\n\nThe scope of this work focuses on proposing the use of a pre-trained model and its performance will be decided then based on the accuracy of detection of fake images.\n\nPreprocessed images with labels real and fake will be fed to the pre-trained models for feature extraction using ELA and then PCA would be used for feature reduction. Later the extracted features will be used for the classification of images using different classifiers. The proposed work will be using python and the CASIA V2.0 dataset. Figure 1.5 shows the high-level approach that we will use in this study.\n\n\n\nFigure 1.5 Flow Chart for proposed approach\n\n\n\n1.6 Structure of Study\n\n\n\nThis work is majorly classified into six chapters.\n\nThe first chapter is an introductory chapter that covers what is image forgery, how image forgery can be categorized and achieved, what issues does image forgery cause and how to detect image forgery and scope of this report. The second chapter covers all the related work done in the field of image forensics and have also covered details about origin of digital photography, formats of images and all the techniques developed so far in detecting image forgeries. The third chapter discusses dataset description and the proposed research methodology which covers preprocessing methods and existing models. The fourth chapter is an extension of chapter three and covers in-depth the EDA performed and preprocessing techniques implementation and model building. The fifth chapter discusses the results and evaluation metrics. The sixth chapter discusses conclusion, learnings and future recommendation. The last section covers the references.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCHAPTER 2: LITERATURE REVIEW\n\n\n\n\n\n2.1 Introduction\n\n\n\nThis chapter covers the basics of digital photography, including the production process, image formats, and file compression. It also covers some of the techniques and procedures used to forge digital images, as well as some of the techniques and methods used to detect image forgery.\n\nDigital Photography\n\nThe method of capturing light and recording the captured light to a digital medium from electromagnetic spectrum is known as digital photography. A sensor in a digital camera store and saves photographic images in digital form.\n\nThe camera captures millions of \"dots\" called pixels in digital images, which a computer uses to show the image by splitting the image into a corresponding grid of pixels, each comprising RGB (Red, Blue, Green) areas called sub-pixels. The brightness of each of these sub-pixels is then specified using the values saved in the digital snapshot, and the pixel's combined brightness is viewed as a single colour (Dennis P Curtin, 2011).\n\n\n\nFigure 2.1 Digital Photography\n\nThe digital photography model is depicted in Figure 8. An optical lens [Figure 2.1(b)] focuses upon the light from the scene [Figure 2.1(a)] into the camera. The quantity of light that enters the camera and the quantity of the scene captured, as well as focusing on the image are all controlled by the lens. The lenses are classified into three groups: Wide-angle lenses capture a large arc of the scene's content and do have a shorter focal length than a standard lens. Macro lenses photograph objects that are extremely close to the lens. Telephoto lenses (Wikipedia, 2021) capture images from a great distance.\n\nThe light passes through a succession of filters after passing through the lens, therefore prepares it for conversion into the digital realm [Figure 2.1(c)]. The anti-aliasing filter, sitting on top of a camera's sensor and preventing high spatial frequencies from going through, which corresponds to very tiny details in the scene, is one of these filters. The anti-aliasing filter, on the other hand, restricts the frequencies that pass through the sensor and is commonly referred to as a \"low-pass\" filter since it only allows lower frequencies to pass through(Anderson, 2011).\n\nThe image sensor is made up of pixels, which are picture elements that register the amount of light that falls on them. After that, the elements convert the amount of light they received into the corresponding number of electrons, which are subsequently translated into voltage and finally into digital values. CCD (Charge-Coupled Devise) and CMOS (Complementary Metal-oxide Semiconductor) are the two types of sensors typically used in cameras. Although their basic functions are comparable, most digital cameras employ a CMOS sensor since it provides faster speed and reduced power usage (Moynihan, 2011).\n\nA Colour Filter Array (CFA) is employed to help discriminate between different frequency ranges of light because pixels are not colour sensitive elements [Figure 2.1(d)]. The colour filter array (CFA) is a mosaic of microscopic colour filters placed on top of monochrome image sensors, commonly on top of CCD and CMOS sensors, to split the light into distinct pixels by colour frequency. The Bayer RGB colour filter array, which consists of a mosaic of red, green, and blue, is one of the most prevalent forms of filter arrays. The filter pattern is 50 percent green, 25 percent blue, and 25 percent red since human eyes are more sensitive to green colours (Lukac and Plataniotis, 2005)\n\nFollowing the CFA, the digital sensor collects the information by quantizing light intensity levels, which converts a continuous signal into a finite set of intensity values. Following the sensor's conversion of light intensity for each colour, a \"mosaicing\" method is required to compute colour values for each of the three basic colours represented at each pixel location using interpolation [Figure 2.1(f)], (Kimmel, 1998)\n\nThe onboard camera processor then performs several actions, including white balance and gamma correction [Figure 2.1(f)]. [Figure 2.1(g)] The camera processes the sensor data and creates a digital image file for storing on a digital memory device.\n\n\n\n\n\n\n\n2.2 Image Compression\n\n\n\nConsider a general CT picture, with 512 x 512 pixels size and has a pixel resolution of 12 bits per pixel. The CT picture would require a minimum of 393,216 bytes or 3,145,728 bits of storage space, ignoring any further metadata. When you consider the amount of storage required for a 100-slice study and ten such studies per week, it's simple to see why digital storage capacity is such a major worry when working with digital imaging. Rather than mindlessly purchasing gear to enhance accessible storage capacity, it makes sense to find a way to make better use of what we already have.\n\nThe goal behind picture compression is to take advantage of redundancies in image data and re-encode it in such a way that it becomes more compact. Lossless and lossy compression are the two basic methods of data compression.\n\n2.2.1 Lossless\n\n\n\nFrom (Karam, n.d.) Lossless compression reduces mathematical redundancy by compressing data without sacrificing quality. This implies the algorithm scans the photos for recurrent patterns or sequences and simplifies them to a simple formula. In a typical CT image, for example, the surrounding boundaries are usually completely black (air), which is represented by a continuous string of zeros (0 0 0 0 0 0 0 0 0 0 ...). The lossless method would recognise the pattern and replace it with an encoded formula of the type \"repeat zero 10 times\" as shown in Figure 2.2. When a compressed image is decompressed for viewing, the resulting image is identical to the original source image, implying that no data was lost.\n\n\n\nFigure 2.2 Line profile A–extended A's stream of zeros (black) will easily give significant compression savings, however, line profile B–seemingly B's random profile will see a modest benefit. Data patterns and repetitions are exploited in lossless compression. Tan e\n\nWhen the compressed image is decompressed for viewing, the resulting image is identical to the original source image, indicating that no data was lost during compression. Lossless formats save about 1:2 in space and work best with very simple images like schematics and line art, as well as images with clean lines and flat colour.\n\n 2.2.2 Lossy\n\n\n\nPerceptual redundancy is reduced by lossy compression. This means that the algorithm considers the limits of the human eye and discards data that is regarded non-essential to the overall image's perceived quality. Colour details, as well as very bright and very dark tones, for example, are less sensitive to the human eye. The lossy method may then diminish the spatial resolution of the colour channels and smooth the image's highly brilliant and very dark areas.\n\nIt's worth noting that, unlike lossless compression, the final decoded lossy image isn't exactly the same as the original source. The more forceful the compression, the more information is lost, and the difference between the compressed and original image becomes more visible. Lossy compression, on the other hand, usually produces better results, with a space savings of around 1:10 or so. Lossy compression works best with photographic images or images with few sharp edges, such as gradients and tones (Figure 2.3).\n\n\n\nFigure 2.3 Lossy compression artifacts (a, b, and c): the example image is saved at successively greater levels of compression, initially at 1:1, then second image with 1:10, and final image with 1:30 of compression level. The image is quite legible even at 1:30, though there are apparent visual flaws; (d) the image with 1:30 compression level when zoomed 5 times. image is showing visible smudging, which is a really important feature of the lossy compression of the JPEG. (Tan, 2006)\n\n\n\n2.3 Image Formats\n\n\n\nWhen capturing photographs, one of the most crucial workflow decisions you make is which image file format to utilize. A standard specification for encoding information about an image into bits of data for storage is known as an image file format. (Tan, 2006) describes an image saved and encoded in a recognized image format identifies itself as an image and offers essential information like matrix size and bit depth. There are so many different pictures file formats that it might be difficult to figure out which one is ideal for your purposes.\n\nSome, like TIFF, are suitable for printing, while others, like JPEG and PNG, are best for web graphics. We'll go over some of the most prevalent image file formats.\n\n2.3.1 JPEG\n\n\n\nJoint Photographic Experts Group\n\n(Wiggins et al., 2001) JPEG was developed in the early 1990s to be the forerunner of the next generation of image compression techniques. JPEG is the name of the compression method developed by the Independent JPEG Group, not a file format. JPEG, like GIF, employs pixel files to hold bit-mapped information; however, it does not use indexed colour. The JPEG format's strength is its ability to compress large image files greatly, allowing for faster electronic movement. The fundamental flaw \n\nJPEG is that it is a lossy compression technology, which means that data is lost with each compression, potentially resulting in image degradation.\n\n2.3.2 TIFF\n\n\n\nTagged Image File Format\n\n(Wiggins et al., 2001) TIFF is a trademark that was first registered by Aldus, which amalgamated with Adobe Systems later on (San Jose, Calif). TIFF was developed primarily by manufacturers of input and output devices such as printers, monitors, and scanners; as a result, it is designed to work with a variety of image processing devices. TIFF stands for \"Tagged Image File Format,\" which refers to the format's sophisticated file structure. The file's initial header is followed by \"chunks\" of data known as \"tags,\" which send picture information to the computer that displays the file.\n\nTIFF's major advantage is that it can handle a wide range of picture sizes, resolutions, and colour resolution without losing the detail thanks to lossless compression.\n\n2.3.3 BMP\n\n\n\nWindows Bitmap Format\n\nMicrosoft® introduced BMP as the native bitmap picture format for their Microsoft Windows® operating system. BMP is a relatively simple format that lacks many of the capabilities of other, more powerful formats. However, it is supported by most applications and is sometimes referred to as the lowest common denominator format for transferring images across programs. (Tan, 2006)\n\n2.4 Methods of detecting image tampering:\n\n\n\nThe set of image forensic tools used earlier as stated by (Farid, 2009) can be grouped into 5 categories namely\n\n• Pixel-based techniques that detect statistical abnormalities introduced at the pixel level \n\n• Physics-based techniques that explicitly model and detect anomalies in the three-dimensional interaction between physical objects, light, and the camera\n\n• Format-based techniques that leverage the statistical correlations introduced by a specific lossy compression scheme\n\n• Camera-based techniques that exploit artifacts introduced by the camera lens, sensor, or on-chip postprocessing \n\n• Geometry-based techniques that explicitly model and detect anomalies in the three-dimensional interaction between physical objects, light, and the camera\n\n\n\n2.4.1 Physics-based techniques\n\n\n\nTo detect photos that have been manipulated and to discover changing regions in an image, the simplest method is to employ human observation or physical laws. To do so, we can look for elements in the image that are:\n\n2.4.1.1 Lighting and shadows:\n\nBecause shadows and sharp highlights reveal the source and direction of light when multiple photos are combined into a single image, the shadows may be uneven or the lighting may differ. As seen in Figure 2.4 a recognised counterfeit of John Kerry and Jane Fonda sharing the stage at an anti-war rally is shown above. Kerry's estimated light direction is 123, while Fonda's estimated light direction is 86. An actual photograph of Richard Nixon and Elvis Presley is shown here. Nixon and Presley's estimated directions are 98 and 93, respectively.\n\n\n\nFigure 2.4 A recognised counterfeit of Jane Fonda & John Kerry sharing the stage at an anti-war rally is shown above. Kerry's estimated light direction is 123, while Fonda's light direction is 86 estimated. An actual photograph of Richard Nixon and Elvis Presley (Johnson, 2005)\n\n2.4.1.2 Floating and Scale: \n\nWhen you combine too many photographs, the sizes may become erratic. Splicing a person into an image can also make it appear as if that person is floating or not rooted to the earth.\n\n2.4.2 Format Based \n\n\n\n2.4.2.1 Metadata Analysis:\n\n\n\nAs previously stated, images can be saved in a variety of image file formats, each of which contains some information. Changes to the image can sometimes alter the file format in image forgeries. (N.Krawetz, 2007) stated in his paper for the JPEG file format that JPEG comprises feature sets, and therefore changing the image will change the feature set. As a result, if the feature sets are manipulated, those manipulated feature sets can help in identifying tampering.\n\nThe majority of image files contain more than just a picture. They also carry information about the image (metadata). Metadata describes a photograph's provenance, such as the type of camera used, colour space information, and application notes. Different sorts of metadata are included in different photo formats. Beyond the image size and colour space, several formats, such as BMP, PPM, and PBM, contain very little information. A JPEG from a camera, on the other hand, typically carries a lot of information, such as the camera's manufacturer and model, focal and aperture information, and timestamps.\n\nUnless the image was converted from a JPEG or altered, PNG files often contain relatively little information. Some information that we must look for as stated in (Afsal Villan et al., 2017) are:\n\nThe manufacturer, model, and software: These are used to identify the device or application that took the photo. The EXIF metadata block on most digital cameras includes a Make and Model. The camera's firmware version or application information may be described in the software.\n\nImage Dimensions The dimensions of the image are frequently recorded in the metadata. Is the rendered picture size (at the bottom of the metadata) consistent with the other sizes? Many programs crop or resize images without altering the metadata.\n\nDate and time stamps: Look for fields that contain timestamp information. These are used to determine when a photograph was taken or edited. Do the timestamps correspond to the expected timeline?\n\nDifferent types of metadata: There are numerous sorts of metadata. Some are created solely by cameras, while others are created solely by software.\n\nDescriptive text: Many photos have embedded annotations that describe the image, identify the people in it, and so on.\n\nMissing Metadata: Are there any metadata fields that are missing? If the photo was taken with a digital camera, it should include camera-specific data. Metadata is stripped from several programs and internet services. A lack of relevant metadata usually suggests that the image has been resaved rather than being original.\n\nAltered Metadata: The chain of custody for evidence handling is equivalent to metadata. It can tell you how a photo was created, processed, and saved. Some people, on the other hand, modify metadata on purpose. In an attempt to deceive, they may alter timestamps or photo information.\n\n\n\nFigure 2.5 Block diagram of the ELA method used by (Afsal Villan et al., 2017)\n\n\n\n\n\nFigure 2.5 shows the flow chart of the system that is used by (Afsal Villan et al., 2017). This system uses metadata analyser as feature extractor and the features extracted by metadata analyser is used in combination with feature extracted through compression error level analysis. Results from both the components is used for final output.\n\n\n\nAlthough metadata can provide useful information about a photograph, it does have several limitations:\n\nBecause some metadata fields are plain text, photo editors may attempt to modify the metadata, which may go undetected.\n\nMetadata information that is misleading. Concatenating two separate photos in Photoshop, for example, can result in the resultant image containing metadata from one of the concatenated images. Because the metadata information didn't match the new stored image, this results in hazy details.\n\n\n\n2.4.2.2 JPEG Quantization using DCT:\n\n\n\nMost of the images used these days are in JPEG format. Author (Lin et al., 2009b) suggested the use of the DQ effect. DQ effect is the presence of periodic peaks and valleys in the histograms of DCT coefficients.\tThe method uses a DCT block of 8X8. For each frequency, the DCT(Karam, n.d.) coefficients of all the blocks can be gathered to build a histogram. Though this approach was limited to only JPEG format since this method worked without fully decompressing the image, this method was faster. Figure 2.5 shows how DCT can be used to highlight tampered regions.\n\n\n\nFigure 2.6  a. Shows unchanged region of a tampered image as blank region and the changed or forged region as shaded region. \n\nWith further improvement, for localization of tampered region (Wang et al., 2014) proposed the use of Laplacian Mixture model for the distribution of AC DCT coefficient and later EM (Expectation-Maximization) algorithm was employed to find the probability of an image being tampered.\n\n(Huang et al., 2011) DCT is applied to each block to reflect its features in an upgraded DCT technique. Truncating is a technique for reducing the size of features. The feature vectors are then lexicographically sorted, and duplicated image blocks in the sorted list will be new neighbours. In the matching process, duplicated image blocks will be compared. The proposed method is capable of detecting tampering via JPEG compression, blurring, or additive white Gaussian noise. (Alahmadi et al., 2017) used DCT in combination with LBP (Local binary pattern) for tampered part detection.\n\n\n\n2.7 Block diagram of DCT method (Alahmadi et al., 2017)\n\n\n\n2.4.3 Camera-based techniques:\n\n\n\n2.4.3.1 Colour Filter Array Pattern\n\n\n\nA colour filter array (CFA) or colour filter mosaic (CFM) is a mosaic of small colour filters that are put over the pixel sensors of an image sensor to collect colour information in photography.\n\nIn the CFA demosaicing artifacts, tampering leaves a trace in the image. The manipulated region can be determined by identifying these modifications. CFA number pattern estimate was proposed in (Dirik and Memon, 2009). It required the usage of several candidate patterns obtained through picture re-interpolation. For each candidate pattern, the Mean Square Error between the input and interpolated image is calculated. If all of the MSE values are the same, the image has been tampered.  Figure 2.6 and Figure 2.7 illustrate how altered regions can be identified.\n\n\n\nFigure 2.8 Original Image\n\n\n\nFigure 2.9 Tampered region detected using CFA pattern estimation (Regina et al., 2010)\n\nIt is also suggested in (Armas Vega et al., 2020) that the sensor noise in interpolated pixels should be muted if a picture is interpolated. This is related to the low-pass interpolation's nature. Sensor noise variance in interpolated pixels is much lower than the sensor's noise power in non-interpolated pixels. As a result, the interpolation algorithm's artifacts may be quantified by comparing the ratio of interpolated and non-interpolated pixels' noise variances. It's safe to conclude that the input image was modified if this ratio is close to 1.\n\nThis method is used to detect tampering that occurs as a result of modifications including resizing, recompression, and filtering. There are, nevertheless, genuine photographs that do not include CFA artifacts. This method cannot be used to verify the authenticity of such photographs.\n\n2.4.3.2 Camera response\n\n\n\nBecause most digital camera sensors are approximately linear, the amount of light measured by each sensor element and the associated final pixel value should have a linear relationship. Most cameras, on the other hand, use pointwise nonlinearity to improve the final image. In (Hsu and Chang, 2010) authors explain how to estimate this mapping, known as a response function, from a single image in their paper. Tampering is detected by looking for differences in the response function across the image.\n\n2.4.4 Geometric based techniques:\n\n\n\nThese techniques are based on the main point, which is the projection of the camera centre onto the image plane, which allows for the measurement of objects in the real world and their position relative to the camera. (Kashyap et al., n.d.)\n\nGrooves in handgun barrels give the shot a twist, which improves accuracy and range. These grooves acquaint the bullet shot with certain markings to some extent, and can thus be used with a specific firearm. Several picture forensic approaches have been developed by the same soul.\n\nPrinciple point and metric measurement detection methods are two types of geometry-based picture forgery detection systems.(Farid, 2009)\n\n2.5 Advance Image tampering detection\n\n\n\n2.5.1 ELA (Error Level Analysis): \n\n\n\nAs mentioned in (Sudiatmika et al., 2019) by storing photos at a given quality level and then evaluating the difference from the compression level, error level analysis is one way for identifying photographs that have been modified. When a JPEG file is saved for the first time, it compresses the image. Most editing software, such as Adobe Photoshop, Gimp, and Adobe Lightroom, enable JPEG compression. If we use image-editing software to reschedule the image, we’ll have to compress it again.\n\n(Bakiah et al., 2015) The image is broken into 8X8 chunks and recompressed individually using ELA at a pre-set error rate of 95%. If the image has not been altered in any way, each square should be given the same quality rating. The degree of error will also be increased during resave procedures. Following resave procedures, the error level potential was reduced, as evidenced by darker ELA results. The square grid may reach its lowest error level after a certain amount of resaving operations. As a result, each resaving procedure may miss out on frequency and specifics. Figure 2.8 depicts ELA in coloured image where ELA is able to detect tampering quite easily, whereas Figure 2.9 depicts weakness of ELA on grey scale images because the precision of error levels is reduced as the degree of information decreases, error levels generated by high frequency components and error levels generated by various quality are indistinguishable.\n\n\n\nFigure 2.10 a) the actual image, b) Forged image with tampering done around eyes and lips and a flower added on the hat, and c) ELA transformed image for the forged image. (Jeronymo et al., 2017)\n\n\n\n\n\nFigure 2.11 a) shows the original image, b) shows the forgery, with a zeppelin in the background and c) shows ELA for the tampered image. (Jeronymo et al., 2017)\n\nLater (Jeronymo et al., 2017) suggested the use of wavelet thresholding as a denoising technique which helps in removing noise without affecting any other component. The approach successfully reduces noise and improves error levels, allowing for better identification of areas of the image where tampering has occurred. In ELA analysis, the Daubechies wavelet transform outperforms the log Gabor technique. The statistical strategy of calculating a threshold value is not ideal, although producing a better approximation. This is due to the fact that error level analysis builds a noise map from the image by definition. Regions with more noise likely to reflect forgeries, but ELA also generates noise in regions with high frequency components, such as hair or borders. Handling high-frequency regions is proposed as future work.\n\n(Sudiatmika et al., 2019) proposed the use of the system that integrate ELA (Error Level Analysis) method and pre-trained model VGG-16, that produced 92.2% training accuracy after using 100 epochs. ELA (Error Level Analysis) uses the difference in compression level between different subsections of an image to identify forgery. (Armas Vega et al., 2020) proposed use of two algorithms for image forgery detection. Error Level Analysis (ELA) algorithm, was used to detect splicing in an image which highlighted those pixels that have a different level of compression whereas another algorithm was based on the quadratic mean error of the Colour Filter Array (CFA) interpolation pattern which determined the level of interpolation in the manipulated image. They used the CASIA V1.0 dataset and got an accuracy of 73.3% with a high-resolution image. The second algorithm didn't prove useful in the case of low-resolution images.\n\nSimilarly (Qurat-Ul-Ain et al., 2021) proposed the use of ELA along with multiple state of art models and found out that VGG 16 yields the best accuracy among all state of art models with a training accuracy of 91.97%.\n\nThe only issue with ELA is that works only on lossy compression.\n\n2.5.2 CNN\n\n\n\nDL combines the extraction and classification phases of features (low, mid, and high level). The method is data-driven and can learn abstract and sophisticated traits, which are required to detect tampered areas. Furthermore, it saves the effort and time required to locate forged characteristics in manipulated photos. Deep learning model training, on the other hand, is difficult and takes a lot of computer power and a lot of data. Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Deep Neural Networks (DNN) are examples of DL models. Among these DL models, Convolutional Neural Networks (CNN) are popular. The convolution layer on CNN functions as a feature extractor and a discriminator.\n\nBy suppressing image content, (Bayar and Stamm, 2016) introduced a novel convolutional layer that can learns image’s tampered properties. Because manipulation changes some local relationships, this layer calculates the relationship between pixels in local structure rather than the content of a picture. This can detect several instances of image tampering. Any detection approach has the drawback of being unable to provide appropriate results for multiple tampering attacks. Furthermore, the majority of the study is focused on JPEG images, with the tampered region being recognised utilising a clue given by the number of JPEG compression processes.\n\n(Zhang et al., 2016) proposed a two-step detection mechanism for faked photos. They partitioned the image into patches initially and then used a Stacked Autoencoder model to learn the features for each patch. The contextual information is added to each patch in the second stage to ensure correct results.\n\n(Salloum et al., 2018) proposed the use of Multi-task Fully Convolutional Network (MFCN) for image splicing localization, it uses 2 branches of multitask learning. One branch is used to learn the surface label, while the other branch is used to learn the edge or boundary of the spliced region. The base network architecture is the FCN VGG-16 architecture with skip connections. Although there was a considerable performance decrease in post-processing processes, this strategy still outperformed several existing solutions.\n\n(Amerini et al., 2017) present a multi-domain convolutional neural network strategy to localise double JPEG compression. One CNN in the spatial domain, one in the frequency domain, and completely connected layers make up a multi-domain CNN. The input to a spatial domain-based CNN is n*n sized patches of RGB colour channels. It is made up of two completely linked layers and two convolutional blocks. Based on the frequency domain The DCT coefficients for each patch are sent into CNN. Based on the frequency domain Two convolutional layers, two pooling layers, and three complete connections make up CNN. Multi-domain CNN combines the outputs of these two networks' fully linked layers and classifies the patch into one of three categories: uncompressed, single, or double compressed.\n\nWhen the image size is small and the image is compressed, detecting median filtering from it is a difficult operation. (Chen et al., 2015) presented a CNN-based strategy for extracting median filtering residuals from images to address this problem. The first CNN layer is a filter layer that decreases interference caused by the presence of edges and textures. The interference is removed, allowing the model to analyse the traces left by median filtering. The method was evaluated using a dataset of 15352 images that was created by combining five image datasets.\n\nThe traces of numerous devices may be spliced together in a spliced image. (Bondi et al., 2017) introduced a CNN-based technique that extracts features related to camera type from image patches to detect tampering based on the traces left by different camera models. The retrieved features are analysed using a clustering technique, and the image is classified as faked or authentic depending on the results. The method was put to the test using a dataset of 2000 photos from various camera models.\n\n(Abdalla, 2019) discussed a copy-move forgery detection using CNN architecture that employs a pre-processing layer but the results were only satisfactory on the passive forged image.\n\n(Jalab et al., 2019) focussed on increasing the accuracy of image splicing detection as well as also on the reduction of feature vector dimensionality by designing a new fractional texture descriptor using DWT. DWT breaks down an input image into multiple sub-images by applying a low and high pass filter. SVM was finally used as a classifier.\n\n(Jaiswal and Srivastava, 2019) used a pre-trained state of art model RESNET-50 for training the model and then further used Multiclass Model using SVM Learner, K-NN, and Naïve Bayes as a classifier which produced accuracy as 70.26%, 59.91%, and 59.91% respectively.\n\n(Kuznetsov, 2019) proposed the use of VGG 16 Convolution Neural Network for identifying image forgery-splicing. The author suggested feeding network architecture patches of an image from the original image and on the border of image splicing, obtaining results with training accuracy of 97.8 % and test accuracy of 96.8%.\n\n(Hsu et al., 2020) proposed the use of CFFN using a network-based pairwise learning model to detect fake and real images. The pairwise learning approach helped in improving the generalization property of DeepFD. The dataset was generated using state of art GAN model. The model was able to detect fake images generated by new GAN models as well.\n\n(Walia, 2021) proposed use of two streams one handcrafted and other deep features.\n\nOne stream uses discrete cosine transform of the image to compute Markov-based features. Another stream uses the luminance channel of YCbCr colorspace for feature extraction. CASIA v1 and CASIA v2 datasets were used. The accuracy achieved is 99.3% using the proposed fusion-based approach.\n\n(Samir et al., 2020) proposed the use of the AlexNet model for the classification of image tampering. Instead of local response normalization, the model uses batch normalization and rectified linear unit (relu) was replaced by maxout activation function and uses SoftMax as a classifier. Dataset used were NIST (Nimble 2017 Challenge Dataset), CASIA v2.0, DVMM and CASIA v1.0 dataset. K folds classification for dividing the dataset into test and train.\n\n2.6 Comparative study of existing forgery detections methods\n\n\n\nTable 2.1 show comparative study of image forgeries techniques developed so far.\n\nTable 2.1 Comparative study of image forgeries techniques\n\nS. No.\n\nPaper Title\n\nMethod Used\n\nPros/Cons\n\n Year of Publication\n\n1\n\nDetection of copy-move forgery in digital image  \n\nDCT\n\nWill not work in noisy image 2003\n\n2003\n\n2\n\nExposing digital forgeries by detecting duplicated image regions(Popescu and Farid, 2004)\n\nPCA\n\nTime Complexity\n\n2004\n\n3\n\nExposing Digital Forgeries by Detecting Inconsistencies in Lighting (Johnson, 2005)\n\nPhysics Based Technique\n\nNot helpful for detecting advanced forgery\n\n2005\n\n4\n\nIdentifying tampered regions using singular value decomposition in Digital image forensics (Xiaobing and Shengmin, 2008)\n\nSVD\n\nWill not work in highly noised and compressed image\n\n2008\n\n5\n\nFast, automatic and fine-grained tampered JPEG image detection via DCT coefficient analysis(Lin et al., 2009b)\n\nDCT\n\nWorks well with JPEG format \n\n2009\n\n6\n\nImage tamper detection based on demosaicing artifacts; Image tamper detection based on demosaicing artifacts(Dirik and Memon, 2009)\n\nCFA\n\nUse single feature and have low error rate\n\n2009\n\n7\n\nImage forgery detection(Farid, 2009)\n\nGeometric Based technique\n\nNot efficient\n\n2009\n\n8\n\nFast copy-move forgery detection(Lin et al., 2009a)\n\nImproved PCA\n\nWorks well with compressed and noisy images\n\n2009\n\n9\n\nImproved DCT-based detection of copy-move forgery in images(Huang et al., 2011)\n\nDCT\n\nWorks well with forgery caused by JPEG compression, blurring or additive white Gaussian noise\n\n2011\n\n10\n\nAn evaluation of popular copy-move Forgery detection approaches (Christlein et al., 2012)\n\nDCT, DWT, KPCA, PCA\n\nHigh performance and computationally effective\n\n2012\n\n11\n\nAn evaluation of Error Level Analysis in image forensics; An evaluation of Error Level Analysis in image forensics(Bakiah et al., 2015)\n\nELA\n\nELA showed reliability with JPEG compression, image splicing and image retouching forgery.\n\n2015\n\n12\n\nMedian Filtering Forensics Based on Convolutional Neural Networks(Chen et al., 2015)\n\nCNN+MFR\n\nGood accuracy for cut paste forgery\n\n2015\n\n13\n\nA deep learning approach to universal image manipulation detection using a new convolutional layer(Bayar and Stamm, 2016)\n\nCNN\n\nCan works on images with multiple manipulation. High Accuracy\n\n2016\n\n14\n\nImage region forgery detection: A deep learning approach(Zhang et al., 2016)\n\nCNN\n\nUses two State of art tampering detection approach\n\n2016\n\n15\n\nFake Image Detection Using Machine Learning(Afsal Villan et al., 2017)\n\nMetadata Analysis\n\nReliable for detecting forgery\n\n2017\n\n16\n\nPassive detection of image forgery using DCT and local binary pattern(Alahmadi et al., 2017)\n\nDCT +LBP\n\nDetect copy–move and splicing forgeries\n\n2017\n\n17\n\nImage forgery detection by semi-automatic wavelet soft-Thresholding with error level analysis(Jeronymo et al., 2017)\n\nELA + Wavelength Thresholding\n\nWorks well with noisy images, not effective in handling high frequency region\n\n2017\n\n18\n\nLocalization of JPEG double compression through multi-domain convolutional neural networks(Amerini et al., 2017)\n\nCNN\n\nWorks well for double compression JPEG\n\n2017\n\n19\n\nTampering Detection and Localization Through Clustering of Camera-Based CNN Features (Bondi et al., 2017)\n\nCNN\n\nAbe to detect camera model traces from image patches\n\n2017\n\n20\n\nImage Splicing Localization using a Multi-task Fully Convolutional Network (MFCN) (Salloum et al., 2018)\n\nCNN+MFCN\n\nMFCN helped in achieving better localization compared to other localization techniques\n\n2018\n\n21\n\nImage forgery detection using error level analysis and deep learning (Qurat-Ul-Ain et al., 2021)\n\nELA + Metadata Analysis\n\nHigh Accuracy\n\n2019\n\n22\n\nConvolutional Neural Network for Copy-Move (Abdalla, 2019)\n\nSVCNN\n\nImproved result for active copy move forgery but only satisfactory results for passive forgery\n\n2019\n\n23\n\nNew texture descriptor based on modified fractional entropy for digital image splicing forgery detection(Jalab et al., 2019)\n\nCNN+AMFE\n\nSuperior detection accuracy and positive and false positive rates were achieved\n\n2019\n\n24\n\nImage Splicing Detection using Deep Residual Network (Jaiswal and Srivastava, 2019)\n\nRESNET\n\nLow Accuracy\n\n2019\n\n25\n\nDigital image forgery detection using deep learning approach (Kuznetsov, 2019) \n\nVGG16\n\nHigh Accuracy\n\n2019\n\n26\n\nPassive Image Forgery Detection Based on the Demosaicing Algorithm and JPEG Compression(Armas Vega et al., 2020)\n\nCFA\n\nHighly efficiency\n\n2020\n\n27\n\nDeep fake image detection based on pairwise learning(Hsu et al., 2020)\n\nCNN\n\nHigh Accuracy on GAN generated dataset\n\n2020\n\n28\n\nOptimization of a pre-trained AlexNet model for detecting and localizing image forgeries(Samir et al., 2020)\n\nAlexNet +Batch Normalisation\n\nModerately effective\n\n2020\n\n29\n\nForged Face Detection using ELA and Deep Learning Techniques (Qurat-Ul-Ain et al., 2021)\n\nELA +CNN\n\nAccurate and efficient\n\n2021\n\n30\n\nFusion of Handcrafted and Deep Features for Forgery Detection in Digital Images (Walia, 2021)\n\nDCT + YCbCr colorspace\n\nHigh Accuracy\n\n2021\n\n\n\n\n\n\n\n\n\n2.7 Summary\n\n\n\nWe covered some basics regarding digital photography, image formats, and file compression in this chapter. We also covered some of the strategies and procedures used to counterfeit digital photographs, as well as some of the techniques and methods used to identify image forgery. In Chapter 3, we'll go into research methodology, including several key tools for detecting picture forgeries and a convolutional neural network approach for classifying bogus and authentic photos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCHAPTER 3: RESEARCH METHODOLOGY\n\n\n\n3.1 Introduction\n\n\n\nDeep learning is widely used in the field of image classification and recognition. For this work, images need to be classified as real and fake. CNN will be taking images as input and processed through various layers for feature extraction. Technically, images are passed through multiple CNN layers and filters. Thus, features extracted via the CNN network are used for training and testing purposes. Test data is used to predict the classes based on the model build using train data.\n\n\n\n3.2 Methodology\n\n\n\nThe purpose of this research is to design and build a model that can effectively and efficiently be able to identify image forgery. We are using CASIA V2.0 dataset for model training and test purpose.\n\nFor the model building, we will be using CNN (State of art model using transfer learning) and ELA for feature extraction. Below is the block diagram to represent flow of information and steps taken for final outcome.\n\n\n\n\n\nFigure 3.1 Proposed Approach\n\n3.2.1 Data Set Description\n\n\n\nCASIA V2 is a dataset for forgery classification.  It contains a total of 12323 coloured images which are divided into 7491 original images and 5123 tampered images. It contains images from 320X240 to 800X600 pixel images. It contains images with JPEG, TIFF, and BMP formats. Tempered images are mostly splicing using Photoshop as well as blurred images, are also introduced. Original images contain an image of the following 9 categories: scene, article, animal, nature, character, architecture, plant, texture, and indoor.\n\nForged images were created using an original image or multiple original images. Region cropped from original is subjected to rotation, resize, or other distortions before pasting on another image. Figure 3.1 show some examples of images present in CASIA V2.0 dataset.\n\n\n\nFigure 3.2 Example of images captured from CASIA v 2.0 dataset\n\nBelow is the statistical information about the forged/tampered image.\n\nTable 3.1 Statistical information about the Tampered image (Jing Dong, 2013)\n\nCategory\n\nNo of Images\n\nJPEG Format\n\n2064\n\nTIFF Format\n\n3059\n\nSource of tampered region\n\nSame Image\n\n3274\n\n\n\nDifferent Image\n\n1849\n\n\n\nRotation\n\n568\n\n\n\nResize\n\n1648\n\n\n\nDistortion\n\n196\n\n\n\nRotation and resize\n\n532\n\n\n\nResize and Distortion\n\n211\n\n\n\nRotation and Distortion\n\n42\n\n\n\nRotation, Distortion, and Resize\n\n83\n\nManipulation without pre-processing\n\n1843\n\nManipulation\n\nwith\n\npost-processing\n\nBlurring along spliced edges\n\n848\n\n\n\nBlurring on other regions\n\n131\n\nManipulation without post-processing (Blurring)\n\n4144\n\nSize of Tampered Region\n\nSmall\n\n3358\n\n\n\nMedium\n\n819\n\n\n\nLarge\n\n946\n\n\n\n\n\n\n\n\n\n3.2.2 Pre-Processing\n\n\n\nPreprocessing of image: This is a very important step when dealing with image classification problems. So firstly, we will be resizing the image and check if any rotation or orientation change is required for any image. Secondly, we will be applying normalization on the image because CNN learns by back-propagating through various weight matrices throughout the network, thus when the learning rate is applied on all the images then each correction will different each image proportionately.\n\nAnother important thing that needs to be kept in mind is the removal of noise. Denoising is a very important step before any further action is taken.\n\n3.2.3 Proposed Method\n\n\n\nWe will discuss the deep learning that can be used for identifying the fake and real images\n\n3.2.3.1 Convolutional Neural Network (CNN)\n\nA convolutional neural network is a form of ANN that analyses image inputs and has learnable weights and biases for different regions of the image that may be separated from each other. Convolutional Neural Networks have the advantage of using Spatial and Temporal dependencies of the input images, allowing them to have reduced numbers of weights because of reusability of some parameters. (Tammina, 2019)\n\nIn terms of memory and complexity, this procedure is efficient. Figure 3.3 shows basic architecture of convolutional neural network with each component highlighted.\n\n\n\nFigure 3.3 Architecture of Convolutional Neural Network\n\n\n\nThe following are the basic components of a convolutional neural network:\n\nConvolution Layer: A kernel matrix is passed through the input matrix to build a feature map that is used by the following layer in a convolutional layer. By sliding the Kernel matrix over the input matrix, we perform a mathematical action known as convolution. Every location performs element-by-element matrix multiplication and the results is added onto the feature map.(Nielsen, 2015)\n\nConvolution operation is frequently utilized in a range of fields such as image processing, statistics, and physics. Convolution is a technique that can be used on more than one axis. The convoluted image is calculated as follows if we have a 2-Dimensional image input, I, and a 2-Dimensional kernel filter, K:\n\n𝑆(𝑖,𝑗) = ∑∑𝐼(𝑚, 𝑛)𝑘(𝑖 − 𝑚,𝑗 − 𝑛)\n\n\n\nFigure 3.4 Element-wise multiplication and summation in CNN\n\nPooling layer: Like the Convolutional Layer, the Pooling layer, is responsible for reducing the spatial size of Convolved features. The computational power reduces with reduction in dimensionality. It's also beneficial for extracting rotational  and positional dominant features, which helps keep the model's training process smoothly. (Gitbook, n.d.) \n\nFigure 3.5 Pooling Types\n\nAs shown in Figure 3.5 pooling is categorized into two types: maximum pooling and average pooling. The maximum value from the portion of the image covered by the Kernel is returned by Max Pooling. Average Pooling, on the other hand, returns the average of all the values from the Kernel's section of the image.\n\nMax Pooling also works as a Noise Suppressant. It performs de-noising and dimensionality reduction at the same time. Whereas Average Pooling, reduces dimensionality as part of a noise-suppressing strategy. Thus, we can say that Max pooling is better than Average pooling. (Gholamalinezhad and Khosravi, 2020) \n\nFully Connected Layer: The final layer in a convolutional architecture is a fully linked layer. Following the convolution and pooling layers, it is the final layer. As shown in Figure 3.6, typical neural networks, each neuron in the hidden layer is connected to all neurons in the previous layer, therefore a completely connected layer has neurons that connect to the whole input volume.\n\n\n\nFigure 3.6 Fully Connected Layer (Tammina, 2019)\n\n3.2.3.2 Transfer Learning \n\n\n\nIn this section, we will discuss various methods that have been used and have been proved really helpful. We will discuss two methods, one with the use of a custom model building and another one with the use of transfer learning. (Qurat-Ul-Ain et al., 2021)\n\nVGG -16: \n\nVGG-16 is a well-known image classification model that was trained for the ImageNet Challenge (Russakovsky et al., 2015) with a subset of 1000 classes. The Visual Geometry Group at the University of Oxford proposed the VGG architecture, which is defined by a stack of convolutional layers preceding a Max Pooling layer. It has 13 convolutional layers and 3 FC layers, as well as 3X 3 and 1X1 filters with a stride of 1 pixel. (Simonyan and Zisserman, 2015)\n\nThe reasons why VGG 16 is selected as a method that can be used:\n\nIt has a sequential architecture \n\nRecently, in many papers, it has been shown that VGG 16 has been proved useful in identifying fake images using different tampering styles e.g., copy -move, and splicing.\n\nAlthough there are large number of parameters and long inference time than other architectures like Alex Net, Inception, or VGGNet, VGG-16 can be pruned without any significant change in performance. \n\n\n\nThe architecture of VGG 16\n\n\n\n\n\nFigure 3.7 VGG Architecture (Tammina, 2019)\n\nThe first and second convolutional layers are made up of 64 feature kernel filters, each of which is 33 pixels in size. The dimensions of the input picture (RGB image with depth 3) change to 224x224x64 as it passes through the first and second convolutional layers. The output is then sent to the max-pooling layer with a stride of two.\n\nThe 124 feature kernel filters in the third and fourth convolutional layers have a filter size of 33%. After these two layers, a max-pooling layer with stride 2 is applied, and the output is shrunk to 56x56x128.\n\nConvolutional layers with a kernel size of 33 are used in the fifth, sixth, and seventh levels. 256 feature maps are used in all three. These layers are followed by a stride 2 max pooling layer.\n\nThere are two sets of convolutional layers with kernel sizes of 33rd and thirteenth. There are 512 kernel filters in each of these convolutional layer sets. Following these layers is a max pooling layer with a stride of 1.\n\nThe fourteen and fifteen layers are 4096-unit fully connected hidden layers, followed by a 1000-unit SoftMax output layer (sixteenth layer).\n\nLeveraging Transfer Learning with Pre-Trained Model\n\nThere are numerous options for employing pre-trained models in transfer learning; one of them replaces part of the original architecture's layers while retaining the others. For example, the VGG-16 architecture can be used until the block4 pool layer, and that can then be connected to fully-connected layers with an outputs layer with two outputs. The proposed architecture will include pre-trained (frozen) parameters for the layers 1 to layer 4 pools, that will later connect to a new FC layer.\n\n\n\n3.8 Comparative diagram of Conventional Machine Learning and Transfer Learning\n\n3.2.3.3. Error Level Analysis\n\n\n\nError Level Analysis (ELA) is a technique developed by (N.Krawetz, 2007) which makes use of lossy compression present in forged images. Original images have its own property and any tampering done on these original images will certainly leave some traces of forgery and traces are used by ELA.\n\nIn a nutshell, ELA works by taking a lossy image and recompressing it with a known error rate, after which it computes the absolute difference between the analysed and recompressed image. The following is a formal definition of ELA:\n\nError levels,  where n1 and n2 are row and column indices, can be represented by \n\n\nfor each color channel, where X is the image suspected of forgery and X rc is the recompressed image.\n\n Total error levels are error levels averaged across all color channels, as in \n\n\n\nWhere, i = 1, 2, 3, for a RGB image.\n\nThe error levels linked with the actual pixels shows the difference between images; these error levels, expressed as a percentage change, are directly related to compression loss. The pixel has attained its local minima for error at the stated error rate if the amount of change is modest. If there is a significant amount of change, the pixels are likely not at their local minima and are foreign.\n\n3.2.4 Classification\n\n\n\nWe will try using different classifiers such as SVM, XGBoost, Random Forest, and Naïve Bayes. The classifier that yields max accuracy will be considered for the final model.\n\n3.2.5 Evaluation Metrics\n\n\n\nFor image forgery detection, the significant way of evaluating the performance of a model is by measuring how effectively the model was able to detect local tampered region in an image. The whole model works as a classification problem that segregates fake and real images. Evaluation metrics selection should always be selected based on certain justification; it should not be randomly selected. Since ground truth image is not always available, evaluating on pixel level is difficult, thus we will evaluate on image level. (Al-qershi and Khoo, 2018).\n\nTP (True Positive): Tampered images detected correctly as tampered images.\n\nFP (False Positive): Real images detected wrongly as tampered images.\n\nFN (False Negative): Tampered images falsely missed as real images.\n\nTN (True Negative): Real images detected correctly as real images\n\nSo, for image-level detection, we will make use of the metrics such as the Sum of Absolute Differences (SAD) or Sum of Squared Differences (SSD). Whereas, for pixel-based evaluation, we will use Precision, recall and F1-Score for evaluation purpose.\n\nPrecision can be defined as the probability of tampered pixel identified correctly to total tampered pixels in the ground truth image.\n\n\n\nRecall can be defined as the probability of the tampered pixels identified correctly among total pixels present in ground truth image.\n\n\n\nF_ Measure is the harmonic mean of precision and recall.\n\n\n\nAccuracy can be defined as the probability of total pixels identified correctly.\n\n\n\n3.3 Summary\n\n\n\nThis chapter discusses the approach that we will be following for the implementation. The pre-trained model will be used for classifying images between real and fake images. In the next chapter, we will discuss the implementation and will identify the performance of the model using accuracy as a measure of performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n25\n\n\n\n\n\n2\n","output_type":"stream"}]}]}